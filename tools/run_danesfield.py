#!/usr/bin/env python

"""
Run the Danesfield processing pipeline on an AOI from start to finish.
"""

import configparser
import datetime
import logging
import os
import sys

# import other tools
import generate_dsm
import fit_dtm


def create_working_dir(working_dir):
    """
    Create working directory for running algorithms
    All files generated by the system are written to this directory.
    """
    if not working_dir:
        date_str = str(datetime.datetime.now().timestamp())
        working_dir = 'danesfield-' + date_str.split('.')[0]
    if not os.path.isdir(working_dir):
        os.mkdir(working_dir)
    return working_dir


def parse_config_list(list_str, sep=' ', data_type=int):
    """
    Parse a string-formatted list and turn it into a Python list of the specified data type

    :param list_str: The string version of the list to be parsed
    :type list_str: str

    :param sep: The separator used in ``list_str`` to denote elements
    :type sep: str

    :param data_type: The data type to cast the separated list values into
    :type data_type: type

    :return: The list converted into a native Python list with elements of type ``data_type``
    :rtype: []
    """
    split_strs = list_str.split(sep)

    converted_list = []
    for split_str in split_strs:
        converted_list.append(data_type(split_str))

    return converted_list

# Note: here are the AOI boundaries for the current AOIs
# D1: 747285 747908 4407065 4407640
# D2: 749352 750082 4407021 4407863
# D3: 477268 478256 3637333 3638307
# D4: 435532 436917 3354107 3355520


def main(config_fpath):
    # Read configuration file
    config = configparser.ConfigParser()
    config.read(config_fpath)

    # This either parses the working directory from the configuration file and passes it to
    # create the working directory or passes None and so some default working directory is
    # created (based on the time of creation)
    working_dir = create_working_dir(config['paths'].get('work_dir'))

    aoi_name = config['aoi']['name']
    aoi_bounds = parse_config_list(config['aoi']['bounds'])

    gsd = float(config['params'].get('gsd', 0.25))

    #############################################
    # Run P3D point cloud generation
    #############################################
    # TODO implement running P3D from Docker
    p3d_file = config['paths']['p3d_fpath']

    #############################################
    # Render DSM from P3D point cloud
    #############################################

    dsm_file = os.path.join(working_dir, aoi_name + '_P3D_DSM.tif')
    cmd_args = [dsm_file, '-s', p3d_file, '--bounds']
    cmd_args += list(map(str, aoi_bounds))
    cmd_args += ['--gsd', str(gsd)]
    logging.info("---- Running generate_dsm.py ----")
    logging.debug(cmd_args)
    generate_dsm.main(cmd_args)

    #############################################
    # Fit DTM to the DSM
    #############################################

    dtm_file = os.path.join(working_dir, aoi_name + '_DTM.tif')
    cmd_args = [dsm_file, dtm_file]
    logging.info("---- Running fit_dtm.py ----")
    logging.debug(cmd_args)
    fit_dtm.main(cmd_args)

    #############################################
    # Orthorectify images
    #############################################

    # For each source image (PAN and MSI) call orthorectify.py
    # needs to use the DSM, DTM from above and Raytheon RPC file,
    # which is a by-product of P3D.
    #
    # Note: we may eventually select a subset of input images
    # on which to run this and the following steps

    #############################################
    # Pansharpen images
    #############################################

    # Call gdal_pansharpen.py (from GDAL, not Danesfield) like this:
    #    gdal_pansharpen.py PAN_image MSI_image output_image
    # on each of the pairs of matching PAN and MSI orthorectified
    # images from the step above

    #############################################
    # Convert to 8-bit RGB
    #############################################

    # call msi_to_rgb.py on each of the previous Pansharpened images
    # with the '-b' flag to make byte images

    #############################################
    # Segment by Height and Vegetation
    #############################################

    # Call segment_by_height.py using the DSM, DTM, and *one* of the
    # pansharpened images above.  We've been manually picking the most
    # nadir one.  We could do that here or generalize the code to average
    # or otherwise combine the NDVI map from multiple images.
    # the output here has the suffix _threshold_CLS.tif.

    #############################################
    # UNet Semantic Segmentation
    #############################################

    # Collaborate with Chengjiang Long on what to run here

    #############################################
    # Material Segmentation
    #############################################

    # Collaborate with Matt Purri on what to run here

    #############################################
    # PointNet Geon Extraction
    #############################################

    # Collaborate with Xu Zhang (at Columbia) on what to run here

    #############################################
    # Roof Geon Extraction
    #############################################

    # Collaborate with Zhixin Li (and others at Purdue) on what to run here
    # David Stoup is helping with conda packaging

    #############################################
    # Texture Mapping
    #############################################

    # Collaborate with Bastien Jacquet on what to run here
    # Dan Lipsa is helping with conda packaging


if __name__ == '__main__':
    loglevel = os.environ.get('LOGLEVEL', 'INFO').upper()
    logging.basicConfig(level=loglevel)

    main(sys.argv[1:])
